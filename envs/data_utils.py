import sysfrom scipy import statsimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltdef transform_data(data):    transformed = data    for index, row in transformed.iterrows():        raw = row['raw']        transformed.loc[index, 'cube'] = np.power(raw, (1 / 3))        transformed.loc[index, 'sqt'] = np.sqrt(raw)        np.seterr(divide='ignore')        transformed.loc[index, 'log10'] = np.where(raw > 0, np.log10(raw), 0)        transformed.loc[index, 'ln'] = np.where(raw > 0, np.log(raw), 0)        transformed.loc[index, 'log2'] = np.where(raw > 0, np.log2(raw), 0)    return transformeddef get_ordering(data: pd.DataFrame) -> pd.DataFrame:    return data.groupby([data.columns[0], data.columns[1]]).mean().reset_index().sort_values(by=[data.columns[2]], ascending=True)def shift_data(data, times: int = 1):    ordering = get_ordering(data)    stdev_values = data.groupby([data.columns[0], data.columns[1]])[data.columns[2]].std().reset_index().fillna(0)    data_new = data.copy()    previous = None    for _, name in ordering.iterrows():        if previous is not None:            pre_std = stdev_values.loc[(stdev_values[data.columns[0]] == previous[0]) & (stdev_values[data.columns[1]] == previous[1])][data.columns[2]].tolist()[0]            cur_std = stdev_values.loc[(stdev_values[data.columns[0]] == name[0]) & (stdev_values[data.columns[1]] == name[1])][data.columns[2]].tolist()[0]            data_new.loc[(data_new[data.columns[0]] == name[0]) & (data_new[data.columns[1]] == name[1]), data.columns[2]] += (cur_std + pre_std) * times        previous = name    return data_newdef AR_ol(start, mu, sigmaEta, theta, N):    '''    An Ornsteinâ€“Uhlenbeck procedure.    :param start: the starting value of the series -> max of a <componenten, failure>    :param mu: value to end with -> mean of a <componenten, failure>    :param sigma: the new variance -> std of a <componenten, failure>    :param theta: how fast to converge -> fixed to 0.1    :param N: number of series points to create    :return: generated series    '''    series = [start]    for t in range(N):        series.append(series[-1] + theta * (mu - series[-1]) + np.random.normal(0., sigmaEta))    return seriesdef GARCH(start, sigmaEta, N, beta=0.2):    '''    :param start: the starting value of the series    :param N: number of series points to create    :param beta: how much the previous value influences the new value    :return: generated series    '''    n1 = 100  # the first several observations need to be dropped    n2 = N + n1  # sum of two numbers    alpha = (0.1, 0.3)  # GARCH (1,1) coefficients alpha0 and alpha1    variance = np.random.normal(0, sigmaEta, n2)  # the variance of the series, where    series = [start]    for t in range(n2):        series.append(variance[t] * np.sqrt(alpha[0] + alpha[1] * variance[t-1]**2 + beta * series[t-1]**2))    return seriesdef create_non_stationary_data(model: str, data, ARCH_theta=0.1, N=1000):    '''    :param model: choose between AR_ol, GARCH    :param data: data to work on    :param ARCH_theta:    :param N: number of series points to create    :return: a pandas dataframe with non-stationary series for each <component,failure> combination    '''    def std(x):        return np.std(x, ddof=0)    evaluated_data = data.groupby([data.columns[0], data.columns[1]])[data.columns[2]].agg([max, min, std, 'mean']).reset_index()    non_stationary_series = pd.DataFrame(columns=[data.columns[0], data.columns[1], data.columns[2]])    for i, row in evaluated_data.iterrows():        series = []        sigmaEta = pow(row[4], 2) / 2        if model == 'ARCH':            series = AR_ol(row[2], row[5], sigmaEta, ARCH_theta, N)        elif model == 'GARCH':            series = GARCH(row[5], sigmaEta, N)        else:            print('Model is not provided.')            sys.exit(0)        # saving series in table        for s in series:            new_row = pd.DataFrame({data.columns[0]: row[0], data.columns[1]: row[1], data.columns[2]: s}, index=[0])            non_stationary_series = non_stationary_series.append(new_row, ignore_index=True)        # plot series        plt.plot(series)    plt.show()    return non_stationary_seriesdef perform_ttest(shifted_data: pd.DataFrame) -> pd.DataFrame:    ttest_results = pd.DataFrame(columns=['a', 'b', 'statistic', 'pvalue', 'significant'])    ordering = get_ordering(shifted_data)    data_new_grouped = shifted_data.groupby([shifted_data.columns[0], shifted_data.columns[1]])[shifted_data.columns[2]].apply(list).reset_index()    # evaluation starts here    previous = None    for index, name in ordering.iterrows():        if previous is not None:            pre = data_new_grouped.loc[(data_new_grouped[data_new_grouped.columns[0]] == previous[0]) & (data_new_grouped[data_new_grouped.columns[1]] == previous[1])][data_new_grouped.columns[2]].tolist()[0]            cur = data_new_grouped.loc[(data_new_grouped[data_new_grouped.columns[0]] == name[0]) & (data_new_grouped[data_new_grouped.columns[1]] == name[1])][data_new_grouped.columns[2]].tolist()[0]            result = stats.ttest_ind(pre, cur)            new_row = pd.DataFrame({'a': str(previous), 'b': str(name), 'statistic': result[0], 'pvalue': result[1], 'significant': result[1]<0.025}, index=[0])            ttest_results = ttest_results.append(new_row, ignore_index=True)        previous = name    #ttest_results[ttest_results['statistic']<-0.025][["a", "b", "pvalue"]].to_csv('ttest_results_statisticalSignificant.csv')    #ttest_results.to_csv('ttest_' '_all.csv')    return ttest_results