import sysfrom scipy import statsimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltROUNDED_ACCURACY = 4def transform_data(data: pd.DataFrame) -> pd.DataFrame:    transformed = data    for index, row in transformed.iterrows():        raw = row['raw']        transformed.loc[index, 'cube'] = np.power(raw, (1 / 3))        transformed.loc[index, 'sqt'] = np.sqrt(raw)        np.seterr(divide='ignore')        transformed.loc[index, 'log10'] = np.where(raw > 0, np.log10(raw), 0)        transformed.loc[index, 'ln'] = np.where(raw > 0, np.log(raw), 0)        transformed.loc[index, 'log2'] = np.where(raw > 0, np.log2(raw), 0)    return transformed.round(ROUNDED_ACCURACY)def get_ordering(data: pd.DataFrame) -> pd.DataFrame:    '''    The ordering influences the shifting of the <component,failure> groups.    :param data: the dataset    :return: A data frame with the mean values for each <component,failure> group sorted by the mean value, component name, failure name.    '''    mean_values_of_the_groups = data.groupby([data.columns[0], data.columns[1]])[data.columns[2]].mean().reset_index()    return mean_values_of_the_groups.sort_values(by=[data.columns[2], data.columns[0]], ascending=True)def shift_data(data: pd.DataFrame, spread_multiplication_factor: int = 1):    ordering = get_ordering(data)    stdev_values = data.groupby([data.columns[0], data.columns[1]])[data.columns[2]].std().reset_index().fillna(0)    data_new = data.copy()    previous = None    tie_count = 0    for _, values in ordering.iterrows():        if previous is not None:            # standard deviation            std_pre = stdev_values.loc[(stdev_values[data.columns[0]] == previous[0]) & (stdev_values[data.columns[1]] == previous[1])][data.columns[2]].tolist()[0]            std_cur = stdev_values.loc[(stdev_values[data.columns[0]] == values[0]) & (stdev_values[data.columns[1]] == values[1])][data.columns[2]].tolist()[0]            # mean value            mean_pre = previous[2]            mean_cur = values[2]            # check for ties            if mean_cur == mean_pre:                tie_count += std_cur            # shift the data            spread = (std_pre + std_cur) * spread_multiplication_factor + tie_count            data_new.loc[(data_new[data.columns[0]] == values[0]) & (data_new[data.columns[1]] == values[1]), data.columns[2]] += spread        previous = values    return data_new.round(ROUNDED_ACCURACY)def AR_ol(start, mu, sigmaEta, theta, N):    '''    An Ornsteinâ€“Uhlenbeck procedure.    :param start: the starting value of the series -> max of a <componenten, failure>    :param mu: value to end with -> mean of a <componenten, failure>    :param sigma: the new variance -> std of a <componenten, failure>    :param theta: how fast to converge -> fixed to 0.1    :param N: number of series points to create    :return: generated series    '''    series = [start]    for t in range(N):        series.append(series[-1] + theta * (mu - series[-1]) + np.random.normal(0., sigmaEta))    return seriesdef GARCH(start, sigmaEta, N, beta=0.2):    '''    :param start: the starting value of the series    :param N: number of series points to create    :param beta: how much the previous value influences the new value    :return: generated series    '''    n1 = 100  # the first several observations need to be dropped    n2 = N + n1  # sum of two numbers    alpha = (0.1, 0.3)  # GARCH (1,1) coefficients alpha0 and alpha1    variance = np.random.normal(0, sigmaEta, n2)  # the variance of the series, where    series = [start]    for t in range(n2):        series.append(variance[t] * np.sqrt(alpha[0] + alpha[1] * variance[t-1]**2 + beta * series[t-1]**2))    return seriesdef create_non_stationary_data(model: str, data: pd.DataFrame, ARCH_theta=0.1, N=1000):    '''    :param model: choose between AR_ol, GARCH    :param data: data to work on    :param ARCH_theta:    :param N: number of series points to create    :return: a pandas dataframe with non-stationary series for each <component,failure> combination    '''    def std(x):        return np.std(x, ddof=0)    evaluated_data = data.groupby([data.columns[0], data.columns[1]])[data.columns[2]].agg([max, min, std, 'mean']).reset_index(drop=True)    non_stationary_series = pd.DataFrame(columns=[data.columns[0], data.columns[1], data.columns[2]])    for i, row in evaluated_data.iterrows():        series = []        sigmaEta = pow(row[4], 2) / 2        if model == 'ARCH':            series = AR_ol(row[2], row[5], sigmaEta, ARCH_theta, N)        elif model == 'GARCH':            series = GARCH(row[5], sigmaEta, N)        else:            print('Model is not provided.')            sys.exit(0)        # saving series in table        for s in series:            new_row = pd.DataFrame({data.columns[0]: row[0], data.columns[1]: row[1], data.columns[2]: s}, index=[0])            non_stationary_series = non_stationary_series.append(new_row, ignore_index=True)        # plot series        plt.plot(series)    plt.show()    return non_stationary_series.round(ROUNDED_ACCURACY)def execute_ttest(shifted_data: pd.DataFrame) -> pd.DataFrame:    ttest_results = pd.DataFrame(columns=['component_1', 'failure_1', 'component_2', 'failure_2', 'statistic', 'pvalue'])    ordering = get_ordering(shifted_data)    data_grouped = shifted_data.groupby([shifted_data.columns[0], shifted_data.columns[1]])[shifted_data.columns[2]].apply(list).reset_index()    # evaluation starts here    previous = None    for index, name in ordering.iterrows():        if previous is not None:            # get the values for the previous and current <component, failure> group            values_pre = data_grouped.loc[(data_grouped[data_grouped.columns[0]] == previous[0]) & (data_grouped[data_grouped.columns[1]] == previous[1])][data_grouped.columns[2]].tolist()[0]            values_cur = data_grouped.loc[(data_grouped[data_grouped.columns[0]] == name[0]) & (data_grouped[data_grouped.columns[1]] == name[1])][data_grouped.columns[2]].tolist()[0]            # execute ttest            result = stats.ttest_ind(values_pre, values_cur)            new_row = {'component_1': previous[0], 'failure_1': previous[1],                        'component_2': name[0], 'failure_2': name[1],                        'statistic': result[0], 'pvalue': result[1] if result[1] != np.NaN else 1}            ttest_results = ttest_results.append(new_row, ignore_index=True)        previous = name    #ttest_results.to_csv('152.csv')    return ttest_resultsdef get_distinguishable_combinations(ttest_results: pd.DataFrame, significance_level: float = 0.05) -> [()]:    '''    Returns a list of tuples of <component,failure> groups which are considered to be distinguishable.    :param ttest_results: A dataframe with the ttest results for a gorup pair.    :param significance level: by default 0.05    :return: A list of tuples with distinguishable group pairs.    '''    distinguisable_pairs = ttest_results.loc[(ttest_results['pvalue'] < significance_level), ['component_1', 'failure_1', 'component_2', 'failure_2']]    first_list = list(zip(distinguisable_pairs.component_1, distinguisable_pairs.failure_1))    second_list = list(zip(distinguisable_pairs.component_2, distinguisable_pairs.failure_2))    return list(set(first_list + second_list))def filter_environment(data: pd.DataFrame, component_failure_list: list) -> pd.DataFrame:    '''    Returns a dataframe with only these <component,failure> groups which are in the component_failure_list.    :param data: dataframe to be filtered    :param component_failure_list: a list of tuples, which indicates all <component,failure> groups to be maintained in the dataset.    :return: a filtered dataset    '''    filtered_data = pd.DataFrame(columns=data.columns)    for cf in component_failure_list:        selection = data[((data[data.columns[0]] == cf[0]) & (data[data.columns[1]] == cf[1]))]        filtered_data = pd.concat([filtered_data, selection], sort=False, ignore_index=True)    return filtered_data